+++
# Experience widget.
widget = "experience"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = false  # This file represents a page section.
active = true  # Activate this widget? true/false
weight = 40  # Order that this section will appear.

title = "Experience"
subtitle = ""

# Date format for experience
#   Refer to https://sourcethemes.com/academic/docs/customization/#date-format
date_format = "Jan 2006"

# Experiences.
#   Add/remove as many `[[experience]]` blocks below as you like.
#   Required fields are `title`, `company`, and `date_start`.
#   Leave `date_end` empty if it's your current employer.
#   Begin/end multi-line descriptions with 3 quotes `"""`.

[[experience]]
title = "Data Manager - Data Engineering & Business Intelligence"
company = "PayClip"
company_url = "https://clip.mx/"
location = "Guadalajara, Mexico"
date_start = "2022-09-01"
date_end = ""
description = """
* Design and maintain the analytical platform (OLAP) by leveraging the
   data-engineering (foundational work) & business-intelligence teams (reporting
   systems).
* Align with stakeholders and coordinate with data science managers to provide
  timely, resilient, and scalable solutions for the business.
* Hands-on data engineering manager that leads by example, promoting best
  coding practices and working side-by-side with data engineers when needed.

<p></p>

**Skills**: Data Engineering · Big Data · Python · SQL · Amazon Web Services (AWS) · Leadership · Apache Spark
"""


[[experience]]
title = "Lead Data Engineer"
company = "PayClip"
company_url = "https://clip.mx/"
location = "Guadalajara, Mexico"
date_start = "2022-03-01"
date_end = "2022-09-01"
description = """
  * Hands-on technical leadership role.
  * Data platform design in AWS, Databricks, and Snowflake.
  * Coordinate and mentor the data-engineering team on implementing technical solutions.
  * Lead the technical implementation of the data platform (OLAP environment).
  * Managed a centralized data-team with company-wide stakeholders.
  * Ensure best coding practices, high-quality deliveries, and technical proficiency across the team.

<p></p>

**Skills**: Data Engineering · Machine Learning · Python · SQL · Amazon Web Services (AWS) · Apache Spark
"""

[[experience]]
  title = "Senior Data Engineer"
  company = "PayClip"
  company_url = "https://clip.mx/"
  location = "Guadalajara, Mexico"
  date_start = "2019-09-15"
  date_end = "2022-03-01"
  description = """
  * Development & support of the OLAP environment: Data Lake (e.g., S3, parquet, delta tables) & Warehouse (Snowflake, dimensional data modeling).
  * Data replication (CDC) & integration ETL/ELT development with pyspark.
  * Implemented a serverless architecture in AWS Cloud: Lambda, SQS, etc.
  * Developed a set of internal tools and python libraries (e.g., machine-learning support libraries for data scientists). 
  * Data modeling and business automation.
  * Contribute on R&D of new financial products.
  """
  
[[experience]]
  title = "Senior Data Engineer"
  company = "LeadGenius"
  company_url = "https://www.leadgenius.com/about/"
  location = "Guadalajara, Mexico"
  date_start = "2018-11-01"
  date_end = "2019-09-15"
  description = """
  * Created the data processing pipeline from scratch using Scala & Apache Spark.
  * Created a search interface based on Django and Elasticsearch.
  * Business data-modeling and data-quality assessments.
  """

[[experience]]
  title = "Big Data Consultant"
  company = "Intersys Consulting"
  company_url = "https://www.intersysconsulting.com/"
  location = "Guadalajara, Mexico"
  date_start = "2018-03-01"
  date_end = "2018-10-31"
  description = """
  Proactive member of the data-engineering team and Scala developer with a focus on functional programming, big data, and ML. 
  
  * Contributed to the internal definition of the big-data carrer path.
  * Teaching scala and big-data tools (i.e. Spark) to internal consultants. 
  * POCs using a big-data tech stack (Akka Streaming, Kafka, Cassandra, Spark). 
  """

[[experience]]
  title = "Jr. Data Scientist"
  company = "Crabi"
  company_url = "https://www.crabi.com/"
  location = "Guadalajara, Mexico"
  date_start = "2017-02-01"
  date_end = "2018-10-31"
  description = """
  The pricing model proposal at Crabi is known as UBI (usage-based insurance). This means that the insurance premium is calculated according to the user's behavioral data (generated by telematics devices, mobile app, social network, etc.). The initial work of this model begins with generalized linear models and scales up to machine learning algorithms. Among my main responsibilities:
  
  * Contribute to the development of a risk-model based on behavioral data. 
  * Real-time data processing platform using Spark, Kafka, and Cassandra. 
  """
  
[[experience]]
  title = "Intern Data Scientist"
  company = "Crabi"
  company_url = "https://www.crabi.com/"
  location = "Guadalajara, Mexico"
  date_start = "2016-11-01"
  date_end = "2017-02-01"
  description = """
  I began my internship at Crabi while it was an early-stage startup in R&D. My main responsibilities were:
  
  * Data analytics of IoT devices used to track test-users. 
  * Automated business requirements with Python and R. 
  * POC chatbot for FB-messenger. 
  """

+++
