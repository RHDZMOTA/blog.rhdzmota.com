+++
# Experience widget.
widget = "experience"  # See https://sourcethemes.com/academic/docs/page-builder/
headless = true  # This file represents a page section.
active = false  # Activate this widget? true/false
weight = 40  # Order that this section will appear.

title = "Experience"
subtitle = ""

# Date format for experience
#   Refer to https://sourcethemes.com/academic/docs/customization/#date-format
date_format = "Jan 2006"

# Experiences.
#   Add/remove as many `[[experience]]` blocks below as you like.
#   Required fields are `title`, `company`, and `date_start`.
#   Leave `date_end` empty if it's your current employer.
#   Begin/end multi-line descriptions with 3 quotes `"""`.
[[experience]]
  title = "Senior Data Engineer"
  company = "PayClip"
  company_url = "https://clip.mx/"
  location = "Guadalajara, Mexico"
  date_start = "2019-09-15"
  date_end = ""
  description = """
  * Data processing pipeline with Scala, Python, Airflow, and Apache Spark.
  * External data integration and business automation.
  * Contribute on R&D of new financial products.
  """
  
[[experience]]
  title = "Senior Data Engineer"
  company = "LeadGenius"
  company_url = "https://www.leadgenius.com/about/"
  location = "Guadalajara, Mexico"
  date_start = "2018-11-01"
  date_end = "2019-09-15"
  description = """
  * Created the data processing pipeline from scratch using Scala & Apache Spark.
  * Created a search interface based on Django and Elasticsearch.
  * Business data-modeling and data-quality assessments.
  """

[[experience]]
  title = "Big Data Consultant"
  company = "Intersys Consulting"
  company_url = "https://www.intersysconsulting.com/"
  location = "Guadalajara, Mexico"
  date_start = "2018-03-01"
  date_end = "2018-10-31"
  description = """
  Proactive member of the data-engineering team and Scala developer with a focus on functional programming, big data, and ML. 
  
  * Contributed to the internal definition of the big-data carrer path.
  * Teaching scala and big-data tools (i.e. Spark) to internal consultants. 
  * POCs using a big-data tech stack (Akka Streaming, Kafka, Cassandra, Spark). 
  """

[[experience]]
  title = "Jr. Data Scientist"
  company = "Crabi"
  company_url = "https://www.crabi.com/"
  location = "Guadalajara, Mexico"
  date_start = "2017-02-01"
  date_end = "2018-10-31"
  description = """
  The pricing model proposal at Crabi is known as UBI (usage-based insurance). This means that the insurance premium is calculated according to the user's behavioral data (generated by telematics devices, mobile app, social network, etc.). The initial work of this model begins with generalized linear models and scales up to machine learning algorithms. Among my main responsibilities:
  
  * Contribute to the development of a risk-model based on behavioral data. 
  * Real-time data processing platform using Spark, Kafka, and Cassandra. 
  """
  
[[experience]]
  title = "Intern Data Scientist"
  company = "Crabi"
  company_url = "https://www.crabi.com/"
  location = "Guadalajara, Mexico"
  date_start = "2016-11-01"
  date_end = "2017-02-01"
  description = """
  I began my internship at Crabi while it was an early-stage startup in R&D. My main responsibilities were:
  
  * Data analytics of IoT devices used to track test-users. 
  * Automated business requirements with Python and R. 
  * POC chatbot for FB-messenger. 
  """

+++
